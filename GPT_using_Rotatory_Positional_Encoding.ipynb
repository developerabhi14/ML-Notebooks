{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPRbhkTxfR4eb2h2x9xoxGy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/developerabhi14/ML-Notebooks/blob/main/GPT_using_Rotatory_Positional_Encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "id": "J7uenytQWZK-",
        "outputId": "e15a5a02-27a4-4f12-dfaf-73003564daca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-25 12:55:30--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.3’\n",
            "\n",
            "input.txt.3         100%[===================>]   1.06M  --.-KB/s    in 0.008s  \n",
            "\n",
            "2025-03-25 12:55:31 (130 MB/s) - ‘input.txt.3’ saved [1115394/1115394]\n",
            "\n",
            "step 0: train loss 4.4865, val loss 4.4716\n",
            "step 500: train loss 1.7649, val loss 1.8883\n",
            "step 1000: train loss 1.4919, val loss 1.6887\n",
            "step 1500: train loss 1.3797, val loss 1.5933\n",
            "step 2000: train loss 1.3123, val loss 1.5482\n",
            "step 2500: train loss 1.2624, val loss 1.5224\n",
            "step 3000: train loss 1.2208, val loss 1.4986\n",
            "step 3500: train loss 1.1887, val loss 1.4850\n",
            "step 4000: train loss 1.1598, val loss 1.4941\n",
            "step 4500: train loss 1.1383, val loss 1.4890\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-087a52294a6b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-087a52294a6b>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, idx, max_new_tokens)\u001b[0m\n\u001b[1;32m    216\u001b[0m       \u001b[0midx_cond\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m       \u001b[0;31m# get the predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m       \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_cond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m       \u001b[0;31m# focus only on last time step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m       \u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#becomes (B,C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-087a52294a6b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    190\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0mtoken_emb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_embedding_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B,T,C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m     \u001b[0;31m# positional encoding is now handled by RoPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;31m# pos_emb=self.position_embedding_table(torch.arange(T ,device=device)) #(T,C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2549\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "# hyperparameters\n",
        "batch_size=64\n",
        "block_size=256\n",
        "max_iters=5000\n",
        "eval_interval=500\n",
        "learning_rate=3e-4\n",
        "device='cuda' if torch.cuda.is_available else 'cpu'\n",
        "# device='cpu'\n",
        "eval_iters=200\n",
        "n_embed=384\n",
        "n_head=6\n",
        "n_layer=6\n",
        "dropout=0.2\n",
        "# =====================\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "! wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open(\"input.txt\", \"r\", encoding= \"utf-8\") as f:\n",
        "  text=f.read()\n",
        "\n",
        "# here are all the inique characters that occur inthe text\n",
        "chars=sorted(list(set(text)))\n",
        "vocab_size=len(chars)\n",
        "\n",
        "# Create a mappping from characters to integers\n",
        "stoi={ ch:i for i, ch in enumerate(chars) }\n",
        "itos={ i:ch for i, ch in enumerate(chars) }\n",
        "encode=lambda s: [stoi[c] for c in s] #encoder takes a string, outputs a list of integers\n",
        "decode=lambda l: ''.join([itos[i] for i in l]) # decoder: takes a list of integers, outputs a string\n",
        "\n",
        "# Train and test set splits\n",
        "data=torch.tensor(encode(text), dtype=torch.long)\n",
        "n=int(0.9*len(data)) # first 90% will be train, rest will be val\n",
        "train_data=data[:n]\n",
        "val_data=data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "  # generate a small batch of ata of inputs x and targets y\n",
        "  data=train_data if split==\"train\" else val_data\n",
        "  ix=torch.randint(len(data)-block_size, (batch_size,))\n",
        "  x=torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y=torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x,y=x.to(device), y.to(device)\n",
        "  return x,y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out={}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses=torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      # sample a batch of data\n",
        "      X,Y=get_batch(split)\n",
        "      logits, loss=model(X,Y)\n",
        "      losses[k]=loss.item()\n",
        "    out[split]=losses.mean()\n",
        "  model.train()\n",
        "  return out\n",
        "\n",
        "class RotaryEmbedding(nn.Module):\n",
        "    def __init__(self, dim, max_seq_len=2048):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.max_seq_len = max_seq_len\n",
        "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim)).to(device)\n",
        "        self.register_buffer(\"inv_freq\", inv_freq)\n",
        "\n",
        "    def forward(self, positions):\n",
        "        # positions: [seq_len] or [batch_size, seq_len]\n",
        "        positions=positions.to(device)\n",
        "        if len(positions.shape) == 1:\n",
        "            positions = positions.unsqueeze(0)  # [1, seq_len]\n",
        "        freqs = torch.einsum(\"b n, d -> b n d\", positions.float(), self.inv_freq)\n",
        "        emb = torch.cat((freqs, freqs), dim=-1)\n",
        "        return emb\n",
        "\n",
        "def rotate_half(x):\n",
        "    x1 = x[..., : x.shape[-1] // 2]\n",
        "    x2 = x[..., x.shape[-1] // 2 :]\n",
        "    return torch.cat((-x2, x1), dim=-1)\n",
        "\n",
        "def apply_rotary_pos_emb(q, k, positions):\n",
        "    # q, k: [batch_size, seq_len, head_dim]\n",
        "    emb = RotaryEmbedding(q.shape[-1])(positions.to(q.device))\n",
        "    cos, sin = emb.cos(), emb.sin()\n",
        "    q_rotated = (q * cos) + (rotate_half(q) * sin)\n",
        "    k_rotated = (k * cos) + (rotate_half(k) * sin)\n",
        "    return q_rotated, k_rotated\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "    def __init__(self,head_size):\n",
        "      super().__init__()\n",
        "      self.key=nn.Linear(n_embed, head_size, bias=False)\n",
        "      self.query=nn.Linear(n_embed, head_size, bias=False)\n",
        "      self.value=nn.Linear(n_embed, head_size, bias=False)\n",
        "      self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "      self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "      B,T,C=x.shape\n",
        "      k=self.key(x)\n",
        "      q=self.query(x)\n",
        "\n",
        "      positions = torch.arange(T)\n",
        "      q, k = apply_rotary_pos_emb(q, k, positions)\n",
        "\n",
        "      # compute attention scores (\"affinities\")\n",
        "      wei=q @ k.transpose(-2,-1) * C**-0.5  # (B,T,C) @ (B,C,T) ----> (B,T,T)\n",
        "      wei=wei.masked_fill(self.tril[:T, :T]==0, float('-inf')) # (B,T,T)\n",
        "      wei=F.softmax(wei, dim=-1) #(B,T,T)\n",
        "      wei=self.dropout(wei)\n",
        "      # perform the weighted aggregation of the values\n",
        "      v=self.value(x) #(B,T,C)\n",
        "      out=wei@v #(B,T,T) @ (B,C,T) ---> (B,T,C)\n",
        "      return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  \"\"\" multiple heads of self attention in parallel \"\"\"\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads=nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj=nn.Linear(head_size*num_heads, n_embed)\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out=torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    out=self.dropout(self.proj(out))\n",
        "    return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  \"\"\" a simple linear layer followed by a non linearity \"\"\"\n",
        "\n",
        "  def __init__(self, n_embed):\n",
        "    super().__init__()\n",
        "    self.net=nn.Sequential(\n",
        "      nn.Linear(n_embed, 4*n_embed),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(4*n_embed, n_embed),\n",
        "      nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "  \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "  def __init__(self, n_embed, n_head):\n",
        "    super().__init__()\n",
        "    head_size=n_embed//n_head\n",
        "    self.sa=MultiHeadAttention(n_head, n_head)\n",
        "    self.ffwd=FeedForward(n_embed)\n",
        "    self.ln1=nn.LayerNorm(n_embed)\n",
        "    self.ln2=nn.LayerNorm(n_embed)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=x+self.sa(self.ln1(x))\n",
        "    x=x+self.ffwd(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table=nn.Embedding(vocab_size, n_embed)\n",
        "    self.position_embedding_table=nn.Embedding(block_size, n_embed)\n",
        "\n",
        "    # self.sa_head=MultiHeadAttention(4,n_embed//4)# i.e. 4 heads of 8 dimensional self-attention\n",
        "    # self.ffwd=FeedForward(n_embed)\n",
        "\n",
        "    # replace with blocks\n",
        "    # self.blocks=nn.Sequential(\n",
        "    #   Block(n_embed, n_head=4),\n",
        "    #   Block(n_embed, n_head=4),\n",
        "    #   Block(n_embed, n_head=4),\n",
        "    #   nn.LayerNorm(n_embed)\n",
        "    # )\n",
        "    self.blocks=nn.Sequential(*[Block(n_embed, n_head=n_head) for _ in range(n_layer)])\n",
        "    self.lm_head=nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B,T=idx.shape\n",
        "    token_emb=self.token_embedding_table(idx) # (B,T,C)\n",
        "    # positional encoding is now handled by RoPE\n",
        "    # pos_emb=self.position_embedding_table(torch.arange(T ,device=device)) #(T,C)\n",
        "    # x=token_emb+pos_emb # (B,T,C)\n",
        "    x=token_emb\n",
        "    # x=self.sa_head(x) # apply one head of self attention (B,T,C)\n",
        "    # x=self.ffwd(x)\n",
        "\n",
        "    # replace with blocks\n",
        "    x=self.blocks(x) # (B,T,C)\n",
        "    logits=self.lm_head(x)# (B,T, vocab_size)\n",
        "    if targets is None:\n",
        "      loss=None\n",
        "    else:\n",
        "      # pytorch expects()\n",
        "      B,T,C=logits.shape\n",
        "      logits=logits.view(B*T, C)\n",
        "      targets=targets.view(B*T)\n",
        "      loss=F.cross_entropy(logits, targets)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      # crop idz to the last block size tokens\n",
        "      idx_cond=idx[:, -block_size:]\n",
        "      # get the predictions\n",
        "      logits,loss=self(idx_cond)\n",
        "      # focus only on last time step\n",
        "      logits=logits[:,-1,:] #becomes (B,C)\n",
        "      # apply softmax to get probabilities\n",
        "      probs=F.softmax(logits, dim=-1) #(B,C)\n",
        "      # sample the distribution\n",
        "      idx_next=torch.multinomial(probs, num_samples=1) #(B,1)\n",
        "      # append sampled index to the running sequence\n",
        "      idx=torch.cat((idx, idx_next), dim=1) # (B,T+1)\n",
        "    return idx\n",
        "\n",
        "\n",
        "model=BigramLanguageModel()\n",
        "m=model.to(device)\n",
        "\n",
        "optimizer=torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "  if iter%eval_interval ==0:\n",
        "    losses=estimate_loss()\n",
        "    print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "  xb,yb=get_batch('train')\n",
        "  logits,loss=model(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context=torch.zeros((1,1), dtype=torch.long).to(device)\n",
        "print(decode(m.generate(context, max_new_tokens=300)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XZGeA4xXMU_",
        "outputId": "9f3da8ea-590a-49a0-ae1c-0bf7fceee565"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "But with prison: I will see a punity: forewell.\n",
            "\n",
            "COMINIUS:\n",
            "Sir, I knave not, sir there has too love.\n",
            "\n",
            "COMINIUS:\n",
            "And, no more honest I might perfect my gracious come.\n",
            "But, then, marry, I shall have been a clothed men's\n",
            "Trief: come to-day, to venge come to hence the office.\n",
            "\n",
            "CORIOLANUS:\n",
            "Your good tong\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AgEEX2WTjpYo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}